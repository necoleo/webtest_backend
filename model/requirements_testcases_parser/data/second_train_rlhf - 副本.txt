run sh: `/home/ubuntu/webtest_rlhf_project/model_rlhf/bin/python3 /home/ubuntu/webtest_rlhf_project/model_rlhf/lib/python3.10/site-packages/swift/cli/rlhf.py --rlhf_type kto --model Qwen/Qwen-7B-Chat --resume_from_checkpoint /home/ubuntu/webtest_rlhf_project/first_rlhf_model_output/v0-20251010-124038/checkpoint-6 --train_type lora --dataset /home/ubuntu/webtest_rlhf_project/train_data/second_train_data/second_train_data.jsonl --val_dataset /home/ubuntu/webtest_rlhf_project/train_data/second_train_data/second_dev_data.jsonl --num_train_epochs 10 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --learning_rate 3e-5 --lora_rank 8 --lora_alpha 32 --target_modules all-linear --gradient_accumulation_steps 16 --eval_steps 3 --save_steps 3 --save_total_limit 5 --logging_steps 2 --max_length 4096 --output_dir second_rlhf_model_output --warmup_ratio 0.05 --dataloader_num_workers 4 --deepspeed zero3 --offload_optimizer true --offload_model true --undesirable_weight 5.5 --gradient_checkpointing true`
[INFO:swift] Successfully registered `/home/ubuntu/webtest_rlhf_project/model_rlhf/lib/python3.10/site-packages/swift/llm/dataset/data/dataset_info.json`.
[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1
[INFO:swift] Downloading the model from ModelScope Hub, model_id: Qwen/Qwen-7B-Chat
[INFO:swift] Loading the model using model_dir: /home/ubuntu/.cache/modelscope/hub/models/Qwen/Qwen-7B-Chat
[INFO:swift] Setting torch_dtype: torch.bfloat16
[INFO:swift] Because len(args.val_dataset) > 0, setting split_dataset_ratio: 0.0
[INFO:swift] Setting args.lazy_tokenize: False
[INFO:swift] Using deepspeed: {'fp16': {'enabled': 'auto', 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': 'auto'}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'none', 'pin_memory': True}, 'offload_param': {'device': 'none', 'pin_memory': True}, 'overlap_comm': False, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'zero_quantized_weights': False, 'zero_quantized_gradients': False, 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': 2000, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False}
[INFO:swift] output_dir: /home/ubuntu/webtest_rlhf_project/train_script/second_rlhf_model_output/v0-20251030-014714
[INFO:swift] Global seed set to 42
[INFO:swift] args: RLHFArguments(
_n_gpu=-1,
acc_strategy=token,
accelerator_config={'dispatch_batches': False},
adafactor=False,
adalora_beta1=0.85,
adalora_beta2=0.85,
adalora_deltaT=1,
adalora_init_r=12,
adalora_orth_reg_weight=0.5,
adalora_target_r=8,
adalora_tfinal=0,
adalora_tinit=0,
adam_beta1=0.9,
adam_beta2=0.95,
adam_epsilon=1e-08,
adapter_act=gelu,
adapter_length=128,
adapters=[],
add_version=True,
agent_template=None,
aligner_lr=None,
async_generate=False,
attn_impl=None,
auto_find_batch_size=False,
average_tokens_across_devices=True,
batch_eval_metrics=False,
beta=0.1,
bf16=True,
bf16_full_eval=False,
bnb_4bit_compute_dtype=torch.bfloat16,
bnb_4bit_quant_storage=None,
bnb_4bit_quant_type=nf4,
bnb_4bit_use_double_quant=True,
boft_block_num=0,
boft_block_size=4,
boft_dropout=0.0,
boft_n_butterfly_factor=1,
cached_dataset=[],
center_rewards_coefficient=None,
channels=None,
check_model=True,
ckpt_dir=None,
cliprange=0.2,
cliprange_value=0.2,
columns={},
completion_length_limit_scope=per_round,
cosine_max_len=None,
cosine_max_len_value_correct=0.5,
cosine_max_len_value_wrong=0.0,
cosine_min_len_value_correct=1.0,
cosine_min_len_value_wrong=-0.5,
cpo_alpha=1.0,
create_checkpoint_symlink=False,
custom_dataset_info=[],
custom_register_path=[],
data_parallel_size=None,
data_seed=42,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
dataset=['/home/ubuntu/webtest_rlhf_project/train_data/second_train_data/second_train_data.jsonl'],
dataset_num_proc=1,
dataset_shuffle=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=18000000,
debug=None,
deepspeed={'fp16': {'enabled': 'auto', 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': 'auto'}, 'zero_optimization': {'stage': 3, 'offload_optimizer': {'device': 'none', 'pin_memory': True}, 'offload_param': {'device': 'none', 'pin_memory': True}, 'overlap_comm': False, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto', 'zero_quantized_weights': False, 'zero_quantized_gradients': False, 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_gather_16bit_weights_on_model_save': True}, 'gradient_accumulation_steps': 'auto', 'gradient_clipping': 'auto', 'steps_per_print': 2000, 'train_batch_size': 'auto', 'train_micro_batch_size_per_gpu': 'auto', 'wall_clock_breakdown': False},
deepspeed_autotp_size=None,
delta=None,
desirable_weight=1.0,
device_map=None,
disable_tqdm=None,
do_eval=False,
do_predict=False,
do_train=False,
download_mode=reuse_dataset_if_exists,
ds3_gather_for_generation=True,
dynamic_sample=False,
epsilon=0.2,
epsilon_high=None,
eval_accumulation_steps=None,
eval_dataset=[],
eval_dataset_args=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_generation_config=None,
eval_limit=None,
eval_on_start=False,
eval_steps=3.0,
eval_strategy=steps,
eval_use_evalscope=False,
eval_use_gather_object=False,
external_plugins=[],
fourier_n_frequency=2000,
fourier_scaling=300.0,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
freeze_aligner=True,
freeze_llm=False,
freeze_parameters=[],
freeze_parameters_ratio=0.0,
freeze_parameters_regex=None,
freeze_vit=True,
fsdp=,
fsdp_config=None,
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
galore_cos_threshold=0.4,
galore_gamma_proj=2,
galore_optim_per_parameter=False,
galore_proj_bits=4,
galore_proj_group_size=256,
galore_proj_quant=False,
galore_proj_type=std,
galore_quantization=False,
galore_queue_size=5,
galore_rank=128,
galore_scale=1.0,
galore_target_modules=None,
galore_update_proj_gap=50,
galore_with_embedding=False,
gamma=1.0,
gc_collect_after_offload=False,
generation_batch_size=None,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gpu_memory_utilization=None,
gradient_accumulation_steps=16,
gradient_checkpointing=True,
gradient_checkpointing_kwargs=None,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hqq_axis=None,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_revision=None,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_args_error=False,
ignore_data_skip=False,
importance_sampling_level=token,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
init_strategy=None,
init_weights=True,
interleave_prob=None,
jit_mode_eval=False,
kl_coef=0.05,
label_names=None,
label_smoothing=0,
label_smoothing_factor=0.0,
lam=0.95,
lazy_tokenize=False,
learning_rate=3e-05,
length_column_name=length,
liger_kernel_config=None,
limit_mm_per_prompt=None,
lisa_activated_layers=0,
lisa_step_interval=20,
llamapro_num_groups=None,
llamapro_num_new_blocks=4,
lmbda=0.5,
load_args=False,
load_best_model_at_end=False,
load_data_args=False,
load_from_cache_file=True,
local_rank=-1,
local_repo_path=None,
local_rollout_forward_batch_size=64,
log_completions=False,
log_entropy=False,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/home/ubuntu/webtest_rlhf_project/train_script/second_rlhf_model_output/v0-20251030-014714/runs,
logging_first_step=True,
logging_nan_inf_filter=True,
logging_steps=2,
logging_strategy=steps,
logprobs=False,
lora_alpha=32,
lora_bias=none,
lora_dropout=0.05,
lora_dtype=None,
lora_ga_batch_size=2,
lora_ga_direction=ArB2r,
lora_ga_iters=2,
lora_ga_max_length=1024,
lora_ga_scale=stable,
lora_ga_stable_gamma=16,
lora_modules=[],
lora_rank=8,
lorap_lr_ratio=None,
loss_scale=last_round,
loss_type=kto,
loss_weights=None,
lr_scheduler_kwargs=None,
lr_scheduler_type=cosine,
max_completion_length=512,
max_epochs=None,
max_grad_norm=1.0,
max_length=4096,
max_memory={},
max_model_len=None,
max_new_tokens=512,
max_pixels=None,
max_resample_times=3,
max_steps=-1,
max_turns=None,
metric=None,
metric_for_best_model=loss,
missing_eos_penalty=None,
model=Qwen/Qwen-7B-Chat,
model_author=None,
model_kwargs={},
model_name=None,
model_revision=None,
model_type=qwen,
modules_to_save=[],
move_model_batches=None,
mp_parameters=,
multi_turn_func=None,
multi_turn_scheduler=None,
neftune_noise_alpha=None,
new_special_tokens=[],
no_cuda=False,
norm_bbox=None,
num_beams=1,
num_generations=8,
num_iterations=1,
num_labels=None,
num_mini_batches=1,
num_ppo_epochs=4,
num_sample_generations=10,
num_train_epochs=10.0,
offload_model=True,
offload_optimizer=True,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
optimizer=None,
output_dir=/home/ubuntu/webtest_rlhf_project/train_script/second_rlhf_model_output/v0-20251030-014714,
overlong_filter=False,
overwrite_output_dir=False,
packing=False,
padding_free=False,
padding_side=right,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
predict_with_generate=False,
prediction_loss_only=False,
problem_type=None,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
quant_bits=None,
quant_method=None,
ray_scope=last,
ref_model=None,
ref_model_mixup_alpha=0.6,
ref_model_revision=None,
ref_model_sync_steps=512,
ref_model_type=None,
reft_args=None,
reft_intervention_type=LoreftIntervention,
reft_layer_key=None,
reft_layers=None,
reft_rank=4,
remove_unused_columns=True,
repetition_max_penalty=-1.0,
repetition_n_grams=3,
repetition_penalty=1.0,
report_to=['tensorboard'],
response_length=512,
response_prefix=None,
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=/home/ubuntu/webtest_rlhf_project/first_rlhf_model_output/v0-20251010-124038/checkpoint-6,
resume_only_model=False,
reward_adapters=[],
reward_funcs=[],
reward_model=None,
reward_model_plugin=None,
reward_model_revision=None,
reward_model_type=None,
reward_weights=None,
rlhf_type=kto,
rope_scaling=None,
router_aux_loss_coef=0.0,
rpo_alpha=1.0,
run_name=/home/ubuntu/webtest_rlhf_project/train_script/second_rlhf_model_output/v0-20251030-014714,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=3.0,
save_strategy=steps,
save_total_limit=5,
scale_rewards=True,
seed=42,
seq_kd=False,
sequence_parallel_size=1,
sft_alpha=0,
shuffle_buffer_size=1000,
simpo_gamma=1,
skip_memory_metrics=True,
sleep_level=0,
soft_cache_length=None,
soft_max_length=None,
sortish_sampler=False,
split_dataset_ratio=0.0,
steps_per_generation=None,
stop_words=[],
stopping_strategy=first_exhausted,
stream=False,
streaming=False,
strict=False,
swanlab_exp_name=None,
swanlab_lark_secret=None,
swanlab_lark_webhook_url=None,
swanlab_mode=cloud,
swanlab_project=None,
swanlab_token=<SWANLAB_TOKEN>,
swanlab_workspace=None,
sync_ref_model=False,
system=None,
target_modules=['all-linear'],
target_parameters=None,
target_regex=None,
task_type=causal_lm,
teacher_adapters=[],
teacher_model=None,
teacher_model_revision=None,
teacher_model_type=None,
temperature=0.9,
template=qwen,
template_backend=swift,
tensor_parallel_size=None,
tf32=None,
top_entropy_quantile=1.0,
top_k=50,
top_logprobs=None,
top_p=0.9,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_dtype=torch.bfloat16,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
train_dataloader_shuffle=True,
train_type=lora,
trainable_parameters=[],
trainable_parameters_regex=None,
truncation_strategy=delete,
tuner_backend=peft,
undesirable_weight=5.5,
use_async_engine=None,
use_chat_template=True,
use_cpu=False,
use_dora=False,
use_galore=False,
use_hf=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_logits_to_keep=None,
use_mps_device=False,
use_rslora=False,
use_swift_lora=False,
use_vllm=False,
val_dataset=['/home/ubuntu/webtest_rlhf_project/train_data/second_train_data/second_dev_data.jsonl'],
val_dataset_shuffle=False,
vera_d_initial=0.1,
vera_dropout=0.0,
vera_projection_prng_key=0,
vera_rank=256,
vf_coef=0.1,
vit_gradient_checkpointing=None,
vit_lr=None,
vllm_data_parallel_size=1,
vllm_disable_custom_all_reduce=True,
vllm_enable_expert_parallel=False,
vllm_enable_prefix_caching=True,
vllm_enforce_eager=False,
vllm_gpu_memory_utilization=0.9,
vllm_limit_mm_per_prompt={},
vllm_max_lora_rank=16,
vllm_max_model_len=None,
vllm_max_num_seqs=256,
vllm_mode=colocate,
vllm_pipeline_parallel_size=1,
vllm_quantization=None,
vllm_server_base_url=None,
vllm_server_host=None,
vllm_server_port=[8000],
vllm_server_timeout=240.0,
vllm_tensor_parallel_size=1,
vllm_use_async_engine=False,
wandb_log_unique_prompts=None,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.1,
whiten_rewards=False,
zero_hpz_partition_size=None,
)
[INFO:swift] Downloading the model from ModelScope Hub, model_id: Qwen/Qwen-7B-Chat
[INFO:swift] Loading the model using model_dir: /home/ubuntu/.cache/modelscope/hub/models/Qwen/Qwen-7B-Chat
[INFO:swift] model_kwargs: {'device_map': None}
Downloading Model from https://www.modelscope.cn to directory: /home/ubuntu/.cache/modelscope/hub/models/Qwen/Qwen-7B-Chat
Downloading Model from https://www.modelscope.cn to directory: /home/ubuntu/.cache/modelscope/hub/models/Qwen/Qwen-7B-Chat
[2025-10-30 01:47:16,036] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-10-30 01:47:17,573] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 1
[2025-10-30 01:47:17,574] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-10-30 01:47:17,574] [INFO] [comm.py:684:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2025-10-30 01:47:17,976] [INFO] [comm.py:739:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.3.0.13, master_port=29500
[2025-10-30 01:47:17,976] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Try importing flash-attention for faster inference...
Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary
Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm
Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention
[2025-10-30 01:47:18,951] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 259, num_elems = 7.72B
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:04,  1.65it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:03,  1.61it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:03,  1.58it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:02<00:02,  1.62it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:03<00:01,  1.64it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:03<00:01,  1.66it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:04<00:00,  1.67it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.88it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.72it/s]
[INFO:swift] model_info: ModelInfo(model_type='qwen', model_dir='/home/ubuntu/.cache/modelscope/hub/models/Qwen/Qwen-7B-Chat', torch_dtype=torch.bfloat16, max_model_len=8192, quant_method=None, quant_bits=None, rope_scaling=None, is_moe_model=False, config=QWenConfig {
  "architectures": [
    "QWenLMHeadModel"
  ],
  "attn_dropout_prob": 0.0,
  "auto_map": {
    "AutoConfig": "configuration_qwen.QWenConfig",
    "AutoModelForCausalLM": "modeling_qwen.QWenLMHeadModel"
  },
  "bf16": true,
  "emb_dropout_prob": 0.0,
  "fp16": false,
  "fp32": false,
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 22016,
  "kv_channels": 128,
  "layer_norm_epsilon": 1e-06,
  "max_position_embeddings": 32768,
  "model_type": "qwen",
  "no_bias": true,
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "onnx_safe": null,
  "pad_token_id": 151643,
  "rotary_emb_base": 10000,
  "rotary_pct": 1.0,
  "scale_attn_weights": true,
  "seq_length": 8192,
  "softmax_in_fp32": false,
  "tie_word_embeddings": false,
  "tokenizer_class": "QWenTokenizer",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.4",
  "use_cache": true,
  "use_cache_kernel": false,
  "use_cache_quantization": false,
  "use_dynamic_ntk": true,
  "use_flash_attn": true,
  "use_logn_attn": true,
  "vocab_size": 151936
}
, task_type='causal_lm', num_labels=None)
[INFO:swift] model.generation_config: GenerationConfig {
  "chat_format": "chatml",
  "do_sample": true,
  "eos_token_id": 151643,
  "max_new_tokens": 512,
  "max_window_size": 24000,
  "pad_token_id": 151643,
  "temperature": 0.9,
  "top_p": 0.9
}

[INFO:swift] default_system: 'You are a helpful assistant.'
[INFO:swift] max_length: 4096
[INFO:swift] response_prefix: ''
[INFO:swift] agent_template: hermes
[INFO:swift] Start time of running main: 2025-10-30 01:47:23.626102
[INFO:swift] swift.__version__: 3.7.2
/home/ubuntu/webtest_rlhf_project/model_rlhf/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[INFO:swift] train_dataset: Dataset({
    features: ['messages', 'label'],
    num_rows: 31
})
[INFO:swift] val_dataset: Dataset({
    features: ['messages', 'label'],
    num_rows: 2
})
[INFO:swift] [CHOSEN_INPUT_IDS] [151644, 8948, 198, 56568, 110124, 107505, 9370, 103951, 81705, 105503, 1773, 112735, 108965, 21287, 19108, 26232, 81705, 11622, 26355, 3837, 118976, 100873, 104378, 43815, 36407, 101128, 103923, 102802, 33108, 104913, 3837, 101889, 100374, 56568, 32664, 103923, 108894, 101092, 105146, 104378, 43815, 43959, 85106, 9370, 81705, 11622, 26355, 3837, 85106, 101118, 81705, 11622, 26355, 100795, 26381, 3837, 99322, 102844, 100148, 43959, 105444, 9370, 81705, 11622, 26355, 3837, 100148, 100211, 100354, 151645, 198, 151644, 872, 198, 99471, 70074, 31935, 100352, 105051, 115199, 62, 103923, 110165, 58362, 44063, 110917, 64355, 65101, 81812, 101921, 56137, 111446, 100910, 64817, 17447, 63836, 28919, 100354, 102802, 9370, 103923, 104913, 33108, 102802, 104190, 17714, 5122, 99471, 70074, 31935, 100352, 105051, 115199, 62, 103923, 110165, 58362, 44063, 110917, 64355, 65101, 81812, 101921, 56137, 111446, 100910, 64817, 17447, 63836, 1773, 26, 220, 151645, 198, 151644, 77091, 198, 104210, 100354, 101042, 3837, 43959, 87752, 98380, 81705, 11622, 26355, 1773, 81705, 11622, 26355, 5122, 15, 20, 8908, 235, 107, 70074, 31935, 100352, 105051, 115199, 106393, 62, 20002, 64355, 65101, 81812, 101921, 62, 104747, 52334, 5122, 47, 15, 62, 115489, 76095, 5122, 36667, 35415, 72448, 62, 40090, 105652, 5122, 16, 13, 100111, 111446, 100910, 64817, 17447, 63836, 101065, 9480, 29, 17, 13, 72651, 20002, 64355, 65101, 62, 104394, 59151, 5122, 16, 13, 20002, 64355, 65101, 54021, 18493, 64817, 17447, 63836, 9480, 29, 17, 13, 100662, 108888, 64355, 65101, 98380, 105928, 26, 220, 151645]
[INFO:swift] [CHOSEN_INPUT] <|im_start|>system
你是一名资深的软件测试工程师。你需要帮我生成功能测试用例，需要用完整的需求内容来理解业务关联和逻辑，然后结合你对业务的理解针对指定的需求内容生成需要的测试用例，需要考虑测试用例覆盖度，切忌不要生成重复的测试用例，不要创造需求<|im_end|>
<|im_start|>user
药声通销售对话卡片_业务后台需将原有的头像位置调整至顶部栏右上角。。需求关联的业务逻辑和关联规则为：药声通销售对话卡片_业务后台需将原有的头像位置调整至顶部栏右上角。; <|im_end|>
<|im_start|>assistant
基于需求分析，生成以下功能测试用例。测试用例：05 药声通销售对话卡片模块_用户头像位置调整_优先级：P0_前置条件：已登录系统_操作步骤：1.查看顶部栏右上角区域<br>2.点击用户头像_预期结果：1.用户头像显示在右上角<br>2.保持原有头像功能不变; <|im_end|>
[INFO:swift] [CHOSEN_LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 104210, 100354, 101042, 3837, 43959, 87752, 98380, 81705, 11622, 26355, 1773, 81705, 11622, 26355, 5122, 15, 20, 8908, 235, 107, 70074, 31935, 100352, 105051, 115199, 106393, 62, 20002, 64355, 65101, 81812, 101921, 62, 104747, 52334, 5122, 47, 15, 62, 115489, 76095, 5122, 36667, 35415, 72448, 62, 40090, 105652, 5122, 16, 13, 100111, 111446, 100910, 64817, 17447, 63836, 101065, 9480, 29, 17, 13, 72651, 20002, 64355, 65101, 62, 104394, 59151, 5122, 16, 13, 20002, 64355, 65101, 54021, 18493, 64817, 17447, 63836, 9480, 29, 17, 13, 100662, 108888, 64355, 65101, 98380, 105928, 26, 220, 151645]
[INFO:swift] [CHOSEN_LABELS] [-100 * 137]基于需求分析，生成以下功能测试用例。测试用例：05 药声通销售对话卡片模块_用户头像位置调整_优先级：P0_前置条件：已登录系统_操作步骤：1.查看顶部栏右上角区域<br>2.点击用户头像_预期结果：1.用户头像显示在右上角<br>2.保持原有头像功能不变; <|im_end|>
[INFO:swift] [REJECTED_INPUT_IDS] [151644, 8948, 198, 56568, 110124, 107505, 9370, 103951, 81705, 105503, 1773, 112735, 108965, 21287, 19108, 26232, 81705, 11622, 26355, 3837, 118976, 100873, 104378, 43815, 36407, 101128, 103923, 102802, 33108, 104913, 3837, 101889, 100374, 56568, 32664, 103923, 108894, 101092, 105146, 104378, 43815, 43959, 85106, 9370, 81705, 11622, 26355, 3837, 85106, 101118, 81705, 11622, 26355, 100795, 26381, 3837, 99322, 102844, 100148, 43959, 105444, 9370, 81705, 11622, 26355, 3837, 100148, 100211, 100354, 151645, 198, 151644, 872, 198, 99471, 70074, 31935, 100352, 105051, 115199, 62, 103923, 110165, 58362, 44063, 110917, 64355, 65101, 81812, 101921, 56137, 111446, 100910, 64817, 17447, 63836, 28919, 100354, 102802, 9370, 103923, 104913, 33108, 102802, 104190, 17714, 5122, 99471, 70074, 31935, 100352, 105051, 115199, 62, 103923, 110165, 58362, 44063, 110917, 64355, 65101, 81812, 101921, 56137, 111446, 100910, 64817, 17447, 63836, 1773, 26, 220, 151645, 198, 151644, 77091, 198, 104210, 100354, 101042, 3837, 43959, 87752, 98380, 81705, 11622, 26355, 1773, 81705, 11622, 26355, 5122, 15, 20, 8908, 235, 107, 70074, 31935, 100352, 105051, 115199, 106393, 62, 100352, 105051, 100787, 115199, 62, 100352, 117876, 62, 104747, 52334, 5122, 47, 15, 62, 115489, 76095, 5122, 47606, 102574, 9370, 100352, 105051, 62, 40090, 105652, 5122, 16, 13, 100111, 102574, 105051, 9370, 115199, 9480, 29, 17, 13, 101071, 102574, 27369, 62, 104394, 59151, 5122, 16, 13, 54021, 102574, 80094, 9480, 29, 17, 13, 54021, 102574, 9370, 102400, 27369, 9480, 29, 18, 13, 102400, 27369, 102298, 29991, 33108, 81800, 26, 220, 151645]
[INFO:swift] [REJECTED_INPUT] <|im_start|>system
你是一名资深的软件测试工程师。你需要帮我生成功能测试用例，需要用完整的需求内容来理解业务关联和逻辑，然后结合你对业务的理解针对指定的需求内容生成需要的测试用例，需要考虑测试用例覆盖度，切忌不要生成重复的测试用例，不要创造需求<|im_end|>
<|im_start|>user
药声通销售对话卡片_业务后台需将原有的头像位置调整至顶部栏右上角。。需求关联的业务逻辑和关联规则为：药声通销售对话卡片_业务后台需将原有的头像位置调整至顶部栏右上角。; <|im_end|>
<|im_start|>assistant
基于需求分析，生成以下功能测试用例。测试用例：05 药声通销售对话卡片模块_销售对话统计卡片_销售结果显示_优先级：P0_前置条件：存在成交的销售对话_操作步骤：1.查看成交对话的卡片<br>2.检查成交信息_预期结果：1.显示成交金额<br>2.显示成交的药品信息<br>3.药品信息包含名称和数量; <|im_end|>
[INFO:swift] [REJECTED_LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 104210, 100354, 101042, 3837, 43959, 87752, 98380, 81705, 11622, 26355, 1773, 81705, 11622, 26355, 5122, 15, 20, 8908, 235, 107, 70074, 31935, 100352, 105051, 115199, 106393, 62, 100352, 105051, 100787, 115199, 62, 100352, 117876, 62, 104747, 52334, 5122, 47, 15, 62, 115489, 76095, 5122, 47606, 102574, 9370, 100352, 105051, 62, 40090, 105652, 5122, 16, 13, 100111, 102574, 105051, 9370, 115199, 9480, 29, 17, 13, 101071, 102574, 27369, 62, 104394, 59151, 5122, 16, 13, 54021, 102574, 80094, 9480, 29, 17, 13, 54021, 102574, 9370, 102400, 27369, 9480, 29, 18, 13, 102400, 27369, 102298, 29991, 33108, 81800, 26, 220, 151645]
[INFO:swift] [REJECTED_LABELS] [-100 * 137]基于需求分析，生成以下功能测试用例。测试用例：05 药声通销售对话卡片模块_销售对话统计卡片_销售结果显示_优先级：P0_前置条件：存在成交的销售对话_操作步骤：1.查看成交对话的卡片<br>2.检查成交信息_预期结果：1.显示成交金额<br>2.显示成交的药品信息<br>3.药品信息包含名称和数量; <|im_end|>
[INFO:swift] Dataset Token Length: 1240.129032±785.475313, min=235.000000, max=2776.000000, size=31
[INFO:swift] Dataset Token Length: 314.000000±46.000000, min=268.000000, max=360.000000, size=2
[INFO:swift] The RLHFArguments will be saved in: /home/ubuntu/webtest_rlhf_project/train_script/second_rlhf_model_output/v0-20251030-014714/args.json
[INFO:swift] model: PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): QWenLMHeadModel(
      (transformer): QWenModel(
        (wte): Embedding(151936, 4096)
        (drop): Dropout(p=0.0, inplace=False)
        (rotary_emb): RotaryEmbedding()
        (h): ModuleList(
          (0-31): 32 x QWenBlock(
            (ln_1): RMSNorm()
            (attn): QWenAttention(
              (c_attn): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=12288, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=12288, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (c_proj): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (attn_dropout): Dropout(p=0.0, inplace=False)
            )
            (ln_2): RMSNorm()
            (mlp): QWenMLP(
              (w1): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (w2): lora.Linear(
                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=11008, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (c_proj): lora.Linear(
                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=11008, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
          )
        )
        (ln_f): RMSNorm()
      )
      (lm_head): Linear(in_features=4096, out_features=151936, bias=False)
    )
  )
)
[INFO:swift] model_parameter_info: PeftModelForCausalLM: 7739.2159M Params (17.8913M Trainable [0.2312%]), 1.0486M Buffers.
/home/ubuntu/webtest_rlhf_project/model_rlhf/lib/python3.10/site-packages/swift/trainers/mixin.py:94: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `KTOTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
[INFO:swift] use_reentrant: True
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
[INFO:swift] The logging file will be saved in: /home/ubuntu/webtest_rlhf_project/train_script/second_rlhf_model_output/v0-20251030-014714/logging.jsonl
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 16. Using DeepSpeed's value.
Parameter Offload: Total persistent parameters: 6950912 in 289 params
Train:   0%|          | 0/20 [00:00<?, ?it/s]/home/ubuntu/webtest_rlhf_project/model_rlhf/lib/python3.10/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Could not estimate the number of tokens of the input, floating-point operations will not be computed
Train:  35%|███▌      | 7/20 [02:21<04:22, 20.20s/it]